{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Model Evaluation & Comparison\n",
    "\n",
    "Notebook này đánh giá và so sánh các mô hình recommendation đã trained.\n",
    "\n",
    "## Mục Tiêu\n",
    "- Đánh giá 5 models: 3 Content-Based + 2 Collaborative Filtering\n",
    "- Metrics: RMSE, MAE, Precision@K, Recall@K, F1@K, NDCG@K\n",
    "- So sánh performance across models\n",
    "- Identify best model cho từng use case\n",
    "- Visualize kết quả\n",
    "\n",
    "## Models được đánh giá\n",
    "1. Content-Based TF-IDF\n",
    "2. Content-Based Genre\n",
    "3. Content-Based Combined\n",
    "4. Collaborative Filtering Item-Based\n",
    "5. Collaborative Filtering User-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "from models.content_based import ContentBasedRecommender\n",
    "from models.collaborative_filtering import CollaborativeFilteringRecommender\n",
    "from evaluation.metrics import RecommendationMetrics\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"Timestamp: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data & Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "data_dir = '../data/processed'\n",
    "models_dir = '../data/models'\n",
    "\n",
    "movies_path = f'{data_dir}/movies_enriched.csv'\n",
    "ratings_path = f'{data_dir}/ratings.csv'\n",
    "\n",
    "# Load data\n",
    "movies = pd.read_csv(movies_path)\n",
    "ratings = pd.read_csv(ratings_path)\n",
    "\n",
    "print(f\"Loaded {len(movies)} movies, {len(ratings)} ratings\")\n",
    "print(f\"Users: {ratings['userId'].nunique()}\")\n",
    "print(f\"Movies with ratings: {ratings['movieId'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"LOADING TRAINED MODELS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialize models\n",
    "cb_tfidf = ContentBasedRecommender(verbose=False)\n",
    "cb_genre = ContentBasedRecommender(verbose=False)\n",
    "cb_combined = ContentBasedRecommender(verbose=False)\n",
    "cf_item = CollaborativeFilteringRecommender(approach='item', verbose=False)\n",
    "cf_user = CollaborativeFilteringRecommender(approach='user', verbose=False)\n",
    "\n",
    "# Load models\n",
    "print(\"Loading Content-Based TF-IDF...\")\n",
    "cb_tfidf.load_model(f'{models_dir}/content_based_tfidf.pkl', movies_path)\n",
    "\n",
    "print(\"Loading Content-Based Genre...\")\n",
    "cb_genre.load_model(f'{models_dir}/content_based_genre.pkl', movies_path)\n",
    "\n",
    "print(\"Loading Content-Based Combined...\")\n",
    "cb_combined.load_model(f'{models_dir}/content_based_combined.pkl', movies_path)\n",
    "\n",
    "print(\"Loading Collaborative Item-Based...\")\n",
    "cf_item.load_model(f'{models_dir}/collaborative_item_based.pkl', ratings_path, movies_path)\n",
    "\n",
    "print(\"Loading Collaborative User-Based...\")\n",
    "cf_user.load_model(f'{models_dir}/collaborative_user_based.pkl', ratings_path, movies_path)\n",
    "\n",
    "print(\"\\nAll models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PREPARING TEST DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Select test movies (popular movies for better evaluation)\n",
    "popular_movies = movies.nlargest(100, 'num_ratings')\n",
    "test_movie_ids = popular_movies['movieId'].tolist()[:20]\n",
    "\n",
    "print(f\"\\nTest movies: {len(test_movie_ids)}\")\n",
    "print(\"\\nSample test movies:\")\n",
    "print(movies[movies['movieId'].isin(test_movie_ids)][['movieId', 'title_clean', 'genres', 'avg_rating', 'num_ratings']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Content-Based Models Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CONTENT-BASED MODELS EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialize metrics\n",
    "metrics = RecommendationMetrics(verbose=False)\n",
    "\n",
    "# Storage for results\n",
    "cb_results = {\n",
    "    'TF-IDF': {'precision': [], 'recall': [], 'f1': [], 'ndcg': []},\n",
    "    'Genre': {'precision': [], 'recall': [], 'f1': [], 'ndcg': []},\n",
    "    'Combined': {'precision': [], 'recall': [], 'f1': [], 'ndcg': []}\n",
    "}\n",
    "\n",
    "k = 10  # Top-10 recommendations\n",
    "\n",
    "print(f\"\\nEvaluating with K={k}...\\n\")\n",
    "\n",
    "for movie_id in test_movie_ids:\n",
    "    # Get movie's actual genre(s) as ground truth\n",
    "    movie_info = movies[movies['movieId'] == movie_id].iloc[0]\n",
    "    movie_genres = set(movie_info['genres'].split('|'))\n",
    "    \n",
    "    # Find relevant movies (same genre, high rating)\n",
    "    relevant_movies = movies[\n",
    "        (movies['avg_rating'] >= 4.0) &\n",
    "        (movies['num_ratings'] >= 20) &\n",
    "        (movies['movieId'] != movie_id)\n",
    "    ]\n",
    "    relevant_with_genre = relevant_movies[\n",
    "        relevant_movies['genres'].apply(lambda x: len(set(x.split('|')) & movie_genres) > 0)\n",
    "    ]\n",
    "    relevant_ids = relevant_with_genre['movieId'].tolist()\n",
    "    \n",
    "    if len(relevant_ids) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Get recommendations from each model\n",
    "    try:\n",
    "        # TF-IDF\n",
    "        recs_tfidf = cb_tfidf.get_recommendations(movie_id, n=k)\n",
    "        recommended_tfidf = recs_tfidf['movieId'].tolist()\n",
    "        cb_results['TF-IDF']['precision'].append(metrics.precision_at_k(recommended_tfidf, relevant_ids, k))\n",
    "        cb_results['TF-IDF']['recall'].append(metrics.recall_at_k(recommended_tfidf, relevant_ids, k))\n",
    "        cb_results['TF-IDF']['f1'].append(metrics.f1_at_k(recommended_tfidf, relevant_ids, k))\n",
    "        cb_results['TF-IDF']['ndcg'].append(metrics.ndcg_at_k(recommended_tfidf, relevant_ids, k))\n",
    "        \n",
    "        # Genre\n",
    "        recs_genre = cb_genre.get_recommendations(movie_id, n=k)\n",
    "        recommended_genre = recs_genre['movieId'].tolist()\n",
    "        cb_results['Genre']['precision'].append(metrics.precision_at_k(recommended_genre, relevant_ids, k))\n",
    "        cb_results['Genre']['recall'].append(metrics.recall_at_k(recommended_genre, relevant_ids, k))\n",
    "        cb_results['Genre']['f1'].append(metrics.f1_at_k(recommended_genre, relevant_ids, k))\n",
    "        cb_results['Genre']['ndcg'].append(metrics.ndcg_at_k(recommended_genre, relevant_ids, k))\n",
    "        \n",
    "        # Combined\n",
    "        recs_combined = cb_combined.get_recommendations(movie_id, n=k)\n",
    "        recommended_combined = recs_combined['movieId'].tolist()\n",
    "        cb_results['Combined']['precision'].append(metrics.precision_at_k(recommended_combined, relevant_ids, k))\n",
    "        cb_results['Combined']['recall'].append(metrics.recall_at_k(recommended_combined, relevant_ids, k))\n",
    "        cb_results['Combined']['f1'].append(metrics.f1_at_k(recommended_combined, relevant_ids, k))\n",
    "        cb_results['Combined']['ndcg'].append(metrics.ndcg_at_k(recommended_combined, relevant_ids, k))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Calculate averages\n",
    "cb_summary = {}\n",
    "for model_name, metrics_dict in cb_results.items():\n",
    "    cb_summary[model_name] = {\n",
    "        'Precision@10': np.mean(metrics_dict['precision']) if metrics_dict['precision'] else 0,\n",
    "        'Recall@10': np.mean(metrics_dict['recall']) if metrics_dict['recall'] else 0,\n",
    "        'F1@10': np.mean(metrics_dict['f1']) if metrics_dict['f1'] else 0,\n",
    "        'NDCG@10': np.mean(metrics_dict['ndcg']) if metrics_dict['ndcg'] else 0\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "cb_df = pd.DataFrame(cb_summary).T\n",
    "print(\"\\nContent-Based Models Performance:\")\n",
    "print(cb_df)\n",
    "\n",
    "print(f\"\\nEvaluated on {len(cb_results['TF-IDF']['precision'])} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Collaborative Filtering Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"COLLABORATIVE FILTERING EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# For CF, we'll evaluate based on user-movie pairs\n",
    "# Sample some users who have ratings\n",
    "test_users = ratings['userId'].value_counts().head(20).index.tolist()\n",
    "\n",
    "cf_results = {\n",
    "    'Item-Based': {'precision': [], 'recall': [], 'f1': [], 'ndcg': []},\n",
    "    'User-Based': {'precision': [], 'recall': [], 'f1': [], 'ndcg': []}\n",
    "}\n",
    "\n",
    "print(f\"\\nEvaluating {len(test_users)} users...\\n\")\n",
    "\n",
    "for user_id in test_users:\n",
    "    # Get user's highly rated movies as ground truth\n",
    "    user_ratings = ratings[ratings['userId'] == user_id]\n",
    "    relevant_movies = user_ratings[user_ratings['rating'] >= 4.0]['movieId'].tolist()\n",
    "    \n",
    "    if len(relevant_movies) < 3:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # User-based recommendations\n",
    "        recs_user = cf_user.get_user_based_recommendations(user_id, n=k)\n",
    "        recommended_user = recs_user['movieId'].tolist()\n",
    "        \n",
    "        cf_results['User-Based']['precision'].append(metrics.precision_at_k(recommended_user, relevant_movies, k))\n",
    "        cf_results['User-Based']['recall'].append(metrics.recall_at_k(recommended_user, relevant_movies, k))\n",
    "        cf_results['User-Based']['f1'].append(metrics.f1_at_k(recommended_user, relevant_movies, k))\n",
    "        cf_results['User-Based']['ndcg'].append(metrics.ndcg_at_k(recommended_user, relevant_movies, k))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# For item-based, test with popular movies\n",
    "for movie_id in test_movie_ids:\n",
    "    if movie_id not in cf_item.movie_id_to_idx:\n",
    "        continue\n",
    "    \n",
    "    # Get similar movies as ground truth (same high-rated genre)\n",
    "    movie_info = movies[movies['movieId'] == movie_id].iloc[0]\n",
    "    movie_genres = set(movie_info['genres'].split('|'))\n",
    "    \n",
    "    relevant_movies = movies[\n",
    "        (movies['avg_rating'] >= 4.0) &\n",
    "        (movies['num_ratings'] >= 50) &\n",
    "        (movies['movieId'] != movie_id)\n",
    "    ]\n",
    "    relevant_with_genre = relevant_movies[\n",
    "        relevant_movies['genres'].apply(lambda x: len(set(x.split('|')) & movie_genres) > 0)\n",
    "    ]\n",
    "    relevant_ids = relevant_with_genre['movieId'].tolist()\n",
    "    \n",
    "    if len(relevant_ids) == 0:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        recs_item = cf_item.get_item_based_recommendations(movie_id, n=k)\n",
    "        recommended_item = recs_item['movieId'].tolist()\n",
    "        \n",
    "        cf_results['Item-Based']['precision'].append(metrics.precision_at_k(recommended_item, relevant_ids, k))\n",
    "        cf_results['Item-Based']['recall'].append(metrics.recall_at_k(recommended_item, relevant_ids, k))\n",
    "        cf_results['Item-Based']['f1'].append(metrics.f1_at_k(recommended_item, relevant_ids, k))\n",
    "        cf_results['Item-Based']['ndcg'].append(metrics.ndcg_at_k(recommended_item, relevant_ids, k))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Calculate averages\n",
    "cf_summary = {}\n",
    "for model_name, metrics_dict in cf_results.items():\n",
    "    cf_summary[model_name] = {\n",
    "        'Precision@10': np.mean(metrics_dict['precision']) if metrics_dict['precision'] else 0,\n",
    "        'Recall@10': np.mean(metrics_dict['recall']) if metrics_dict['recall'] else 0,\n",
    "        'F1@10': np.mean(metrics_dict['f1']) if metrics_dict['f1'] else 0,\n",
    "        'NDCG@10': np.mean(metrics_dict['ndcg']) if metrics_dict['ndcg'] else 0\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "cf_df = pd.DataFrame(cf_summary).T\n",
    "print(\"\\nCollaborative Filtering Performance:\")\n",
    "print(cf_df)\n",
    "\n",
    "print(f\"\\nItem-Based evaluated on {len(cf_results['Item-Based']['precision'])} test cases\")\n",
    "print(f\"User-Based evaluated on {len(cf_results['User-Based']['precision'])} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Combined Results & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"OVERALL MODEL COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Combine all results\n",
    "all_results = {**cb_summary, **cf_summary}\n",
    "all_df = pd.DataFrame(all_results).T\n",
    "\n",
    "print(\"\\nAll Models Performance:\")\n",
    "print(all_df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Identify best models\n",
    "best_precision = all_df['Precision@10'].idxmax()\n",
    "best_recall = all_df['Recall@10'].idxmax()\n",
    "best_f1 = all_df['F1@10'].idxmax()\n",
    "best_ndcg = all_df['NDCG@10'].idxmax()\n",
    "\n",
    "print(\"Best Models:\")\n",
    "print(f\"  Precision@10: {best_precision} ({all_df.loc[best_precision, 'Precision@10']:.4f})\")\n",
    "print(f\"  Recall@10: {best_recall} ({all_df.loc[best_recall, 'Recall@10']:.4f})\")\n",
    "print(f\"  F1@10: {best_f1} ({all_df.loc[best_f1, 'F1@10']:.4f})\")\n",
    "print(f\"  NDCG@10: {best_ndcg} ({all_df.loc[best_ndcg, 'NDCG@10']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "metrics_to_plot = ['Precision@10', 'Recall@10', 'F1@10', 'NDCG@10']\n",
    "colors = ['steelblue', 'coral', 'lightgreen', 'purple']\n",
    "\n",
    "for idx, (metric, color) in enumerate(zip(metrics_to_plot, colors)):\n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    \n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    values = all_df[metric].values\n",
    "    models = all_df.index.tolist()\n",
    "    \n",
    "    bars = ax.barh(range(len(models)), values, color=color, edgecolor='black')\n",
    "    ax.set_yticks(range(len(models)))\n",
    "    ax.set_yticklabels(models)\n",
    "    ax.set_xlabel(metric)\n",
    "    ax.set_title(f'{metric} Comparison')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, value) in enumerate(zip(bars, values)):\n",
    "        ax.text(value + max(values)*0.01, i, f'{value:.3f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved to reports/model_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar chart for overall comparison\n",
    "from math import pi\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111, projection='polar')\n",
    "\n",
    "categories = ['Precision@10', 'Recall@10', 'F1@10', 'NDCG@10']\n",
    "N = len(categories)\n",
    "\n",
    " # Angle for each metric\n",
    "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "# Plot each model\n",
    "colors_radar = ['b', 'g', 'r', 'c', 'm']\n",
    "for idx, (model_name, color) in enumerate(zip(all_df.index, colors_radar)):\n",
    "    values = all_df.loc[model_name, categories].values.tolist()\n",
    "    values += values[:1]\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=model_name, color=color)\n",
    "    ax.fill(angles, values, alpha=0.15, color=color)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Model Performance Radar Chart', size=16, pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/model_radar_chart.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRadar chart saved to reports/model_radar_chart.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary & Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"EVALUATION SUMMARY & INSIGHTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n1. PERFORMANCE OVERVIEW:\")\n",
    "print(f\"   Best overall model: {all_df.mean(axis=1).idxmax()}\")\n",
    "print(f\"   Average score: {all_df.mean(axis=1).max():.4f}\")\n",
    "\n",
    "print(\"\\n2. CONTENT-BASED INSIGHTS:\")\n",
    "print(f\"   Top CB model: {all_df.loc[['TF-IDF', 'Genre', 'Combined']].mean(axis=1).idxmax()}\")\n",
    "print(\"   - TF-IDF: Good for text similarity\")\n",
    "print(\"   - Genre: Best for genre-based matching\")\n",
    "print(\"   - Combined: Balanced approach\")\n",
    "\n",
    "print(\"\\n3. COLLABORATIVE FILTERING INSIGHTS:\")\n",
    "print(f\"   Top CF model: {all_df.loc[['Item-Based', 'User-Based']].mean(axis=1).idxmax()}\")\n",
    "print(\"   - Item-Based: Better for item-to-item similarity\")\n",
    "print(\"   - User-Based: Better for personalization\")\n",
    "\n",
    "print(\"\\n4. RECOMMENDATIONS:\")\n",
    "print(\"   - For new items: Use Content-Based (no cold-start)\")\n",
    "print(\"   - For established items: Use Collaborative Filtering (better quality)\")\n",
    "print(\"   - For hybrid approach: Combine best of both\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL EVALUATION COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "all_df.to_csv('../reports/model_evaluation_results.csv')\n",
    "print(\"Evaluation results saved to reports/model_evaluation_results.csv\")\n",
    "\n",
    "# Save summary\n",
    "summary_text = f\"\"\"\n",
    "MODEL EVALUATION SUMMARY\n",
    "\n",
    "Best Models:\n",
    "- Precision@10: {best_precision} ({all_df.loc[best_precision, 'Precision@10']:.4f})\n",
    "- Recall@10: {best_recall} ({all_df.loc[best_recall, 'Recall@10']:.4f})\n",
    "- F1@10: {best_f1} ({all_df.loc[best_f1, 'F1@10']:.4f})\n",
    "- NDCG@10: {best_ndcg} ({all_df.loc[best_ndcg, 'NDCG@10']:.4f})\n",
    "\n",
    "Overall Best Model: {all_df.mean(axis=1).idxmax()}\n",
    "\n",
    "Full Results:\n",
    "{all_df.to_string()}\n",
    "\"\"\"\n",
    "\n",
    "with open('../reports/evaluation_summary.txt', 'w') as f:\n",
    "    f.write(summary_text)\n",
    "\n",
    "print(\"Summary saved to reports/evaluation_summary.txt\")\n",
    "print(\"\\nEvaluation complete! Check reports/ folder for detailed results.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RCMsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
